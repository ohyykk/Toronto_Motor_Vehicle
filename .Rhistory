fromJSON(.)$coordinates[[1]][1],
error = function(e) NA
)),
latitude = map_dbl(geometry, ~ tryCatch(
fromJSON(.)$coordinates[[1]][2],
error = function(e) NA
))
) |>
filter(
!is.na(accnum) &
!is.na(date) &
!is.na(road_class) &
!is.na(vehtype) &
!is.na(latitude) &
!is.na(longitude)  # Remove rows with missing critical fields
) |>
filter(
latitude >= -90 & latitude <= 90,  # Validate latitude
longitude >= -180 & longitude <= 180  # Validate longitude
) |>
mutate(
road_class = str_trim(road_class),  # Remove extra spaces
vehtype = str_trim(vehtype)  # Remove extra spaces
) |>
filter(
road_class != "",
impactype != "",
vehtype != ""  # Ensure non-empty values
) |>
distinct(accnum, .keep_all = TRUE) |>  # Ensure unique accident IDs
select(
accnum, date, road_class, vehtype, impactype, rdsfcond, latitude, longitude
) |>  # Select relevant columns
rename(
accident_id = accnum,
road_condition = rdsfcond
)
# Save cleaned traffic data as Parquet
write_parquet(cleaned_traffic_data, here("data", "02-analysis_data", "cleaned_traffic_data.parquet"))
#### Preamble ####
# Purpose: To explore and visualize trends in motor vehicle data, including analyzing accident trends, location-based differences, temporal trends, and injury severity relationships, while leveraging a Bayesian model to assess data patterns and perform posterior predictive checks.
# Author: [Your Name]
# Date: [Current Date]
# Contact: [Your Email]
# License: MIT
# Pre-requisites:
# - Required packages must be installed.
# - Cleaned data should be available in `motor_data.parquet`.
# - Bayesian model file `motor_model.rds` should be available.
#### Workspace setup ####
# Load libraries
library(tidyverse)
library(bayesplot)
library(arrow)
library(here)
# Load data
motor_data <- read_parquet(here("data", "02-analysis_data", "motor_data.parquet"))
#### Preamble ####
# Purpose: To explore and visualize trends in motor vehicle and crime data, including analyzing accident trends, crime trends, temporal trends, and injury severity relationships, while leveraging a Bayesian model to assess data patterns and perform posterior predictive checks.
# Author: [Your Name]
# Date: [Current Date]
# Contact: [Your Email]
# License: MIT
# Pre-requisites:
# - Required packages must be installed.
# - 03-clean_data.R must have been run.
# - 06-model_data.R must have been run.
#### Workspace setup ####
# Load libraries
library(tidyverse)
library(bayesplot)
library(arrow)
library(here)
# Load datasets
crime_data <- read_parquet(here("data", "02-analysis_data", "cleaned_crime_data.parquet"))
traffic_data <- read_parquet(here("data", "02-analysis_data", "cleaned_traffic_data.parquet"))
# Load model
motor_model <- readRDS(file = here::here("models/motor_model.rds"))
#### Preamble ####
# Purpose: Models the relationship between traffic data variables to predict injury severity.
# Author: [Your Name]
# Date: [Current Date]
# Contact: [Your Email]
# License: MIT
# Pre-requisites:
# - The `rstanarm` package must be installed.
# - Cleaned datasets must exist in the specified locations.
# Additional Information: Ensure that `rstanarm` is set up correctly.
#### Workspace setup ####
library(tidyverse)
library(rstanarm)
library(here)
#### Read data ####
# Load cleaned traffic data
traffic_data <- read_parquet(here("data", "02-analysis_data", "cleaned_traffic_data.parquet"))
#### Prepare data ####
# Filter and transform traffic_data for modeling
traffic_model_data <- traffic_data |>
mutate(
injury_severity_numeric = case_when(
impactype == "Fatal" ~ 3,
impactype == "Severe" ~ 2,
impactype == "Minor" ~ 1,
TRUE ~ 0
),
speeding_numeric = ifelse(speeding == "Yes", 1, 0)
) |>
select(injury_severity_numeric, speeding_numeric, latitude, longitude, vehtype, road_class) |>
drop_na()
colnames(traffic_data)
#### Preamble ####
# Purpose: Models the relationship between traffic data variables to predict injury severity.
# Author: [Your Name]
# Date: [Current Date]
# Contact: [Your Email]
# License: MIT
# Pre-requisites:
# - The `rstanarm` package must be installed.
# - Cleaned datasets must exist in the specified locations.
# Additional Information: Ensure that `rstanarm` is set up correctly.
#### Workspace setup ####
library(tidyverse)
library(rstanarm)
library(here)
#### Read data ####
# Load cleaned traffic data
traffic_data <- read_parquet(here("data", "02-analysis_data", "cleaned_traffic_data.parquet"))
#### Prepare data ####
# Filter and transform traffic_data for modeling
traffic_model_data <- traffic_data |>
mutate(
injury_severity_numeric = case_when(
impactype == "Fatal" ~ 3,
impactype == "Severe" ~ 2,
impactype == "Minor" ~ 1,
TRUE ~ 0
),
speeding_numeric = 0  # Placeholder for speeding
) |>
select(injury_severity_numeric, speeding_numeric, latitude, longitude, vehtype, road_class) |>
drop_na()
#### Model data ####
# Bayesian logistic regression for predicting injury severity
injury_model <-
stan_glm(
formula = injury_severity_numeric ~ speeding_numeric + vehtype + road_class + latitude + longitude,
data = traffic_model_data,
family = gaussian(),  # Change to an appropriate family if injury severity is ordinal
prior = normal(location = 0, scale = 2.5, autoscale = TRUE),
prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),
prior_aux = exponential(rate = 1, autoscale = TRUE),
seed = 853
)
#### Preamble ####
# Purpose: To build and save a Bayesian regression model that combines traffic and theft data to predict injury severity.
# Author: [Your Name]
# Date: [Current Date]
# Contact: [Your Email]
# License: MIT
# Pre-requisites:
# - Required packages must be installed.
# - Cleaned traffic and crime datasets must be available in Parquet format.
#### Workspace setup ####
# Load libraries
library(tidyverse)
library(rstanarm)
library(arrow)
library(here)
# Load datasets
traffic_data <- read_parquet(here("data", "02-analysis_data", "cleaned_traffic_data.parquet"))
crime_data <- read_parquet(here("data", "02-analysis_data", "cleaned_crime_data.parquet"))
#### Prepare data ####
# Aggregate features from crime_data for spatial join with traffic_data
crime_aggregated <- crime_data |>
mutate(
latitude_bin = round(latitude, 3),  # Group crimes by latitude bins
longitude_bin = round(longitude, 3) # Group crimes by longitude bins
) |>
group_by(latitude_bin, longitude_bin) |>
summarise(
crime_count = n(),  # Total crimes in the area
avg_offense_severity = mean(as.numeric(factor(offense)), na.rm = TRUE),  # Average severity of offenses
.groups = "drop"
)
# Prepare traffic data and join with aggregated crime data
traffic_model_data <- traffic_data |>
mutate(
injury_severity_numeric = case_when(
impactype == "Fatal" ~ 3,
impactype == "Severe" ~ 2,
impactype == "Minor" ~ 1,
TRUE ~ 0
),
road_class = factor(road_class),
vehtype = factor(vehtype),
latitude_bin = round(latitude, 3),
longitude_bin = round(longitude, 3)
) |>
left_join(crime_aggregated, by = c("latitude_bin", "longitude_bin")) |>
mutate(
crime_count = replace_na(crime_count, 0),  # Replace missing crime data with 0
avg_offense_severity = replace_na(avg_offense_severity, 0)  # Replace missing severity with 0
) |>
select(injury_severity_numeric, road_class, vehtype, latitude, longitude, crime_count, avg_offense_severity) |>
drop_na()
#### Model data ####
# Bayesian regression model
combined_model <-
stan_glm(
formula = injury_severity_numeric ~ vehtype + road_class + latitude + longitude + crime_count + avg_offense_severity,
data = traffic_model_data,
family = gaussian(),
prior = normal(location = 0, scale = 2.5, autoscale = TRUE),
prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),
prior_aux = exponential(rate = 1, autoscale = TRUE),
seed = 304
)
#### Preamble ####
# Purpose: To build and save a logistic regression model that combines traffic and crime data to predict injury severity.
# Author: [Your Name]
# Date: [Current Date]
# Contact: [Your Email]
# License: MIT
# Pre-requisites:
# - Required packages must be installed.
# - Cleaned traffic and crime datasets must be available in Parquet format.
#### Workspace setup ####
# Load libraries
library(tidyverse)
library(arrow)
library(here)
# Load datasets
traffic_data <- read_parquet(here("data", "02-analysis_data", "cleaned_traffic_data.parquet"))
crime_data <- read_parquet(here("data", "02-analysis_data", "cleaned_crime_data.parquet"))
#### Prepare data ####
# Aggregate features from crime_data for spatial join with traffic_data
crime_aggregated <- crime_data |>
mutate(
latitude_bin = round(latitude, 3),  # Group crimes by latitude bins
longitude_bin = round(longitude, 3) # Group crimes by longitude bins
) |>
group_by(latitude_bin, longitude_bin) |>
summarise(
crime_count = n(),  # Total crimes in the area
avg_offense_severity = mean(as.numeric(factor(offense)), na.rm = TRUE),  # Average severity of offenses
.groups = "drop"
)
# Prepare traffic data and join with aggregated crime data
traffic_model_data <- traffic_data |>
mutate(
injury_severity_binary = ifelse(impactype %in% c("Fatal", "Severe"), 1, 0),  # Binary outcome for severe injuries
road_class = factor(road_class),
vehtype = factor(vehtype),
latitude_bin = round(latitude, 3),
longitude_bin = round(longitude, 3)
) |>
left_join(crime_aggregated, by = c("latitude_bin", "longitude_bin")) |>
mutate(
crime_count = replace_na(crime_count, 0),  # Replace missing crime data with 0
avg_offense_severity = replace_na(avg_offense_severity, 0)  # Replace missing severity with 0
) |>
select(injury_severity_binary, road_class, vehtype, latitude, longitude, crime_count, avg_offense_severity) |>
drop_na()
#### Model data ####
# Logistic regression model
combined_model <- glm(
formula = injury_severity_binary ~ vehtype + road_class + latitude + longitude + crime_count + avg_offense_severity,
data = traffic_model_data,
family = binomial()
)
table(traffic_model_data$vehtype)
table(traffic_model_data$road_class)
# Cleaning process for traffic_data
cleaned_traffic_data <- traffic_data |>
janitor::clean_names() |>  # Standardize column names
mutate(
date = as.Date(date, format = "%Y-%m-%d"),
# Extract latitude and longitude from the geometry column if present
longitude = map_dbl(geometry, ~ tryCatch(
fromJSON(.)$coordinates[[1]][1],
error = function(e) NA
)),
latitude = map_dbl(geometry, ~ tryCatch(
fromJSON(.)$coordinates[[1]][2],
error = function(e) NA
))
) |>
filter(
# Ensure critical fields are not missing
!is.na(accnum) &
!is.na(date) &
!is.na(road_class) &
!is.na(impactype) &
!is.na(latitude) &
!is.na(longitude)
) |>
filter(
# Validate latitude and longitude ranges
latitude >= -90 & latitude <= 90,
longitude >= -180 & longitude <= 180
) |>
mutate(
road_class = str_trim(road_class),  # Remove extra spaces
impactype = str_trim(impactype),    # Remove extra spaces
vehtype = str_trim(vehtype)         # Remove extra spaces
) |>
filter(
# Avoid overly aggressive filtering
road_class != "",
impactype != "",
vehtype != ""
) |>
distinct(accnum, date, road_class, impactype, .keep_all = TRUE) |>  # Adjust duplicate removal
select(
accident_id = accnum,
date,
road_class,
vehtype,
impactype,
road_condition = rdsfcond,
latitude,
longitude
)
# Cleaning process for traffic_data
cleaned_traffic_data <- traffic_data |>
janitor::clean_names() |>  # Standardize column names
mutate(
date = as.Date(date, format = "%Y-%m-%d"),
# Extract latitude and longitude from the geometry column
longitude = map_dbl(geometry, ~ tryCatch(
fromJSON(.)$coordinates[[1]][1],
error = function(e) NA
)),
latitude = map_dbl(geometry, ~ tryCatch(
fromJSON(.)$coordinates[[1]][2],
error = function(e) NA
))
) |>
filter(
!is.na(accnum) &
!is.na(date) &
!is.na(road_class) &
!is.na(vehtype) &
!is.na(latitude) &
!is.na(longitude)  # Remove rows with missing critical fields
) |>
filter(
latitude >= -90 & latitude <= 90,  # Validate latitude
longitude >= -180 & longitude <= 180  # Validate longitude
) |>
mutate(
road_class = str_trim(road_class),  # Remove extra spaces
vehtype = str_trim(vehtype)  # Remove extra spaces
) |>
filter(
road_class != "",
impactype != "",
vehtype != ""  # Ensure non-empty values
) |>
distinct(accnum, .keep_all = TRUE) |>  # Ensure unique accident IDs
select(
accnum, date, road_class, vehtype, impactype, road_condition, latitude, longitude
) |>  # Select relevant columns
rename(
accident_id = accnum
)
#### Clean traffic_data ####
# Load raw traffic data
traffic_data <- read_csv(here("data", "01-raw_data", "Motor Vehicle Collisions with KSI Data.csv"))
# Cleaning process for traffic_data
cleaned_traffic_data <- traffic_data |>
janitor::clean_names() |>  # Standardize column names
mutate(
date = as.Date(date, format = "%Y-%m-%d")
) |>
filter(
!is.na(accnum) &
!is.na(date) &
!is.na(road_class) &
!is.na(vehtype)  # Remove rows with missing critical fields
) |>
mutate(
road_class = str_trim(road_class),  # Remove extra spaces
vehtype = str_trim(vehtype)  # Remove extra spaces
) |>
filter(
road_class != "",
impactype != "",
vehtype != ""  # Ensure non-empty values
) |>
distinct(accnum, .keep_all = TRUE) |>  # Ensure unique accident IDs
select(
accnum, date, road_class, vehtype, impactype, road_condition
) |>  # Select relevant columns
rename(
accident_id = accnum
)
# Cleaning process for traffic_data
cleaned_traffic_data <- traffic_data |>
janitor::clean_names() |>  # Standardize column names
mutate(
date = as.Date(date, format = "%Y-%m-%d")
) |>
filter(
!is.na(accnum) &
!is.na(date) &
!is.na(road_class) &
!is.na(vehtype)  # Remove rows with missing critical fields
) |>
mutate(
road_class = str_trim(road_class),  # Remove extra spaces
vehtype = str_trim(vehtype)  # Remove extra spaces
) |>
filter(
road_class != "",
impactype != "",
vehtype != ""  # Ensure non-empty values
) |>
distinct(accnum, .keep_all = TRUE) |>  # Ensure unique accident IDs
select(
accnum, date, road_class, vehtype, impactype  # Select only columns that exist
) |>
rename(
accident_id = accnum
)
# Save cleaned traffic data as Parquet
write_parquet(cleaned_traffic_data, here("data", "02-analysis_data", "cleaned_traffic_data.parquet"))
# Save cleaned traffic data as Parquet
write_parquet(cleaned_traffic_data, here("data", "02-analysis_data", "cleaned_traffic_data.csv"))
#### Clean traffic_data ####
# Load raw traffic data
traffic_data <- read_csv(here("data", "01-raw_data", "Motor Vehicle Collisions with KSI Data.csv"))
# Cleaning process for traffic_data
cleaned_traffic_data <- traffic_data |>
janitor::clean_names() |>  # Standardize column names
select(-geometry) |>  # Remove the geometry column
mutate(
date = as.Date(date, format = "%Y-%m-%d"),  # Convert date column to Date format
road_class = str_trim(road_class),  # Remove extra spaces
vehtype = str_trim(vehtype),  # Remove extra spaces
impactype = str_trim(impactype),  # Remove extra spaces
speeding = ifelse(speeding == "Yes", 1, 0)  # Convert 'speeding' to binary numeric
) |>
filter(
!is.na(accnum) &  # Remove rows with missing accident IDs
!is.na(date) &  # Remove rows with missing dates
!is.na(road_class) &  # Ensure road_class is not missing
!is.na(vehtype) &  # Ensure vehtype is not missing
!is.na(impactype)  # Ensure impactype is not missing
) |>
distinct(accnum, .keep_all = TRUE) |>  # Ensure unique accident IDs
select(  # Select relevant columns
accnum, date, road_class, vehtype, impactype, speeding, fatal_no, injury, division, hood_158, neighbourhood_158
) |>
rename(  # Rename columns for consistency
accident_id = accnum,
neighborhood = neighbourhood_158
)
# Save cleaned traffic data as Parquet
write_parquet(cleaned_traffic_data, here("data", "02-analysis_data", "cleaned_traffic_data.csv"))
#### Clean traffic_data ####
# Load raw traffic data
traffic_data <- read_csv(here("data", "01-raw_data", "Motor Vehicle Collisions with KSI Data.csv"))
# Remove the geometry column
traffic_data <- dataset %>% select(-geometry)
#### Clean traffic_data ####
# Load raw traffic data
traffic_data <- read_csv(here("data", "01-raw_data", "Motor Vehicle Collisions with KSI Data.csv"))
# Remove the geometry column
traffic_data <- traffic_data %>% select(-geometry)
# Clean column names to ensure they are consistent and usable
traffic_data <- traffic_data %>%
rename_all(~ gsub("\\.+", "_", .)) %>% # Replace dots in column names with underscores
rename_all(~ gsub("[^a-zA-Z0-9_]", "", .)) # Remove non-alphanumeric characters
# Remove duplicates if any (based on all columns)
traffic_data <- traffic_data %>% distinct()
# Handle missing values (e.g., replace 'None' with NA for better handling)
traffic_data <- traffic_data %>% mutate_all(~ ifelse(. == "None", NA, .))
#### Clean traffic_data ####
# Load necessary libraries
library(dplyr)
library(readr)
library(arrow)
library(here)
# Load raw traffic data
traffic_data <- read_csv(here("data", "01-raw_data", "Motor Vehicle Collisions with KSI Data.csv"))
# Remove the geometry column
traffic_data <- traffic_data %>% select(-geometry)
# Clean column names to ensure they are consistent and usable
traffic_data <- traffic_data %>%
rename_with(~ gsub("\\.+", "_", .), everything()) %>% # Replace dots in column names with underscores
rename_with(~ gsub("[^a-zA-Z0-9_]", "", .), everything()) # Remove non-alphanumeric characters
# Remove duplicates if any (based on all columns)
traffic_data <- traffic_data %>% distinct()
# Handle missing values (e.g., replace 'None' with NA for better handling)
traffic_data <- traffic_data %>%
mutate(across(where(is.character), ~ ifelse(. == "None", NA, .)))
# Convert appropriate columns to numeric or categorical types
if("INVAGE" %in% names(traffic_data)) {
traffic_data$INVAGE <- as.numeric(traffic_data$INVAGE)
}
if("INJURY" %in% names(traffic_data)) {
traffic_data$INJURY <- as.factor(traffic_data$INJURY)
}
# Save the cleaned dataset (optional)
write_parquet(traffic_data, here("data", "02-analysis_data", "cleaned_traffic_data.parquet"))
# Print a summary of the cleaned dataset
print(summary(traffic_data))
# Save the cleaned dataset (optional)
write_parquet(traffic_data, here("data", "02-analysis_data", "cleaned_traffic_data.csv"))
# Print a summary of the cleaned dataset
print(summary(traffic_data))
